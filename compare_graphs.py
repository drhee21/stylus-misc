import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from readCsv import readResult, readScore

#this graphs genes vs scores, with the ability to put multiple on top of each other to clearly see where they differ. I've been using this with the first file
#being the exhaustive scores, and the rest being the heuristics. In some situations, this means that the exhaustive score can get covered
#You can either make a second graph with just the exhaustive scores under it to compare, or just assume that whichever is the highest score is what the
#exhaustive algorithm says because the heuristics will never surpass the exhaustive
def graphScores(file1, file2 = '', file3 = '', file4 = '', char = -1): 
    #Input: csv files that are archetype/gene score tables (1-3), if multiple, they need to be about the same genes and archetypes, 
    #Output: a graph of the data. as described above the definition line
    g1, a1, d1 = readResult(file1)
    x = g1
    dt1=np.transpose(d1)
    #if you want to graph data from a different archetype, change the 0 to the number that matches that archetype number
    #I can't graph them all because the csv file I'm using has almost 600 genes, so graphing 600 points alone is a lot, I don't want
    #to also multiply that by 11 archetypes
    if char == -1:
        arche=5
    else:
        arche = char
    y = dt1[arche]
    fig, ax = plt.subplots(figsize=(25, 10))
    #ax.figure(figsize = (25, 5))
    ax.plot(x, y, label = file1[:-4])
    ax.set_ylabel('Scores')
    ax.set_xlabel('Genes in arbitrary order')
    ax.set_title('Score of Heuristics and Exhaustive using ' + a1[arche])
    if file2:
        g2, a2, d2 = readResult(file2)
        dt2=np.transpose(d2)
        y = dt2[arche]
        ax.plot(x,y, label = file2[:-4])
    if file3:
        g3, a3, d3 = readResult(file3)
        dt3=np.transpose(d3)
        y = dt3[arche]
        ax.plot(x,y, label = file3[:-4])
    if file4:
        g4, a4, d4 = readResult(file4)
        dt4=np.transpose(d4)
        y = dt4[arche]
        ax.plot(x,y, label = file4[:-4])
    ax.set_xticklabels([])
    plt.legend(loc='upper right')
    plt.show()
    return

#This graph compares heuristic scores to exhaustive scores, a perfect heuristic would be a straight "45 degree" line
def graphExhaustHeurist(exhaustFile, heuristFile, zoom = -1):
    #Input: the exhaustive score csv file and the heuristic score csv file. These are the files generated by my GeneArchetype_Heuristic_Table or the exhaustive one, or a table of similar format. The zoom input is a number so that you can zoom in the graph if you so desire to focus more on the smaller numbers that are more likely incorrect instead of looking at all of the data provided
    #output: a graph of the data, as described above the definition line
    eGene, eArch, eData = readResult(exhaustFile)
    hGene, hArch, hData = readResult(heuristFile)

    exhaust = eData.flatten()
    heurist = hData.flatten()

    if zoom!=-1:
        x = exhaust < zoom
        exhaust = exhaust[x]
        heurist = heurist[x]

    plt.scatter(exhaust, heurist, s=.5)
    plt.xlabel('Exhaustive Scores')
    plt.ylabel(heuristFile[:-4] + ' Scores')
    plt.title('Exhaustive vs Heuristic Scores')
    
    plt.show()
    return
    
def correctChart(exhaustFile, heuristFile, zoom = -1):
    eGene, eArch, eData = readResult(exhaustFile)
    hGene, hArch, hData = readResult(heuristFile)

    exhaust = eData.flatten()
    heurist = hData.flatten()

    if zoom == -1:
        zoom = max(exhaust)
    
    num = round(zoom/.005)
    values = []
    means = []
    stanD = []

    for i in range(0,num):
        a1 = exhaust > (.005*i)
        a2 = exhaust < (.005*(i+1))
        a = a1*a2
        e = exhaust[a]
        h = heurist[a]
        if len(e) > 0:
            values.append((.005*i)+.0025)
            count = 0
            err = []
            for j in range(len(e)):
                x = (e[j]-h[j])/e[j]
                err.append(x)
            means.append(np.mean(err))
            stanD.append(np.std(err))

    plt.xlabel('Scores')
    plt.ylabel('Error')
    plt.title('Error of Heuristic ' + heuristFile[:-4])
    plt.errorbar(values, means, stanD)
    plt.show()

#This creates a plot of the error (if e = ehaustive score and h = heuristic score, then error = (e-h)/h) of the heuristics sorted by their exhaustive scores
#These graphs are all put on one graph, so it's easier to compare.
def compareHeuristics(exhaustFile, heurist1File, heurist2File, heurist3File = '', zoom = -1):
    #Input: a csv of exhaustive scores, and at least 2 csvs of heuristic files. See readResult for more info on the csv format required. Zoom just allows you to
    # only show numbers below a certain point, since those are most commonly incorrect, though it doesn't actually add any more data because this graphs
    # in .01 increments regardless
    #Output: a graph with error bars for standard deviation of the error of at least 2 different heuristics. Basically what I said above the def line
    f3 = False
    if heurist3File:
        f3 = True
    
    eGene, eArch, eData = readResult(exhaustFile)
    h1Gene, h1Arch, h1Data = readResult(heurist1File)
    h2Gene, h2Arch, h2Data = readResult(heurist2File)
    if f3:
        h3Gene, h3Arch, h3Data = readResult(heurist3File) 

    exhaust = eData.flatten()
    heurist1 = h1Data.flatten()
    heurist2 = h2Data.flatten()
    if f3:
        heurist3 = h3Data.flatten()
        mean3 = []
        stanD3 = []
    if zoom ==-1:
        zoom = max(exhaust)
    num = round(zoom/.01)
    values = []
    mean1 = []
    mean2 = []
    stanD1 = []
    stanD2 = []

    for j in range(0,num):
        a1 = exhaust > (j*.01)
        a2 = exhaust < ((j+1)*.01)
        a = a1*a2
         
        e = exhaust[a]
        h1 = heurist1[a]
        h2 = heurist2[a]
        err1 = []
        err2 = []
        if f3:
            h3 = heurist3[a]
            err3 = []
        if len(e) != 0:
            values.append(j*.01+.005)
            for i in range(len(e)):
                x = (e[i]-h1[i])/e[i]
                err1.append(x)
                x = (e[i]-h2[i])/e[i]
                err2.append(x)
                if f3:
                    x = (e[i]-h3[i])/e[i]
                    err3.append(x)
            mean1.append(np.mean(err1))
            stanD1.append(np.std(err1))
            mean2.append(np.mean(err2))
            stanD2.append(np.std(err2))
            if f3:
                mean3.append(np.mean(err3))
                stanD3.append(np.std(err3))

    print("Note: the size of the error bars represents the standard deviation, but the bars are centered around the median on the graph. There aren't any negative errors even if the bars show it")
    #the labels below can be edited to actually say what the heuristics are displaying
    fig, ax = plt.subplots(figsize=(7, 4))
    ax.errorbar(values, mean1, stanD1, label = heurist1File[:-4])
    ax.errorbar(values, mean2, stanD2, label = heurist2File[:-4])
    if f3:
        ax.errorbar(values, mean3, stanD3, label = heurist3File[:-4])
    ax.set_title('Error for different heuristics, with standard dev error bars')
    ax.set_xlabel('Correct score for each gene')
    ax.set_ylabel('Error ((exhaustive-heuristic)/exhaustive)')
    plt.legend(loc='upper right')
    plt.show()
    return
#this graphs the percent of correct values for each heuristic. This is different than the error ones because error gave you some credit if the heuristic
# found a pretty good score, but not the best one. This only counts those that have the entirely correct score and disregards any that fail
#This will just skip over any values that don't have data (so if none of the scores fall between 1.6 and 1.7 then it just won't put a point there), meaning 
# that there could be some really long straight lines if there are large gaps of data
def percentCorrect(exhaustFile, heuristFile, zoom = -1):
    #Input: exhaustive scores csv, heuristic scores csv (see readResult input), zoom allows someone to focus on just a smaller portion of the graph if they so
    # desire, since often the biggest differences are smaller. However, the code corrently checks every .01 no matter how big or small the spread, so zooming
    # in doesn't give you any new data
    #Output: See above the definition, a graph of percent error for different exhaustive score ranges
    
    eGene, eArch, eData = readResult(exhaustFile)
    hGene, hArch, hData = readResult(heuristFile)

    exhaust = eData.flatten()
    heurist = hData.flatten()

    if zoom == -1:
        zoom = max(exhaust)

    num = round(zoom/.01)
    values = []
    percents = []

    for i in range(0,num):
        a1 = exhaust > (.01*i)
        a2 = exhaust < (.01*(i+1))
        a = a1*a2
        e = exhaust[a]
        h = heurist[a]
        if len(e) > 0:
            values.append((.01*i)+.005)
            count = 0
            for j in range(len(e)):
                if e[j] == h[j]:
                    count+= 1
            p = count/len(e) * 100
            percents.append(p)

    plt.xlabel('Scores')
    plt.ylabel('Percents')
    plt.title('Percent Correct of Heuristic ' + heuristFile[:-4])
    plt.errorbar(values, percents)
    plt.show()

    return