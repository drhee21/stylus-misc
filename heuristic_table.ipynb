{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c0f6932-c2f8-4eb4-90a4-99fe2386b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "\n",
    "from compare_genes import getScores\n",
    "from xmlparse import loadRef, loadGeometryBases, getXmlScore, minXml\n",
    "from score_strokes import alignStrokes, strokeErrorMatrix\n",
    "from exhaustive import computeExhaustive, exhaustScore, exhaustScoreAlignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe2079-2815-4401-9fb5-0d7019ae8b87",
   "metadata": {},
   "source": [
    "## Scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63c298f2-495a-46fd-96bd-c8c2f6f32384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(ref_char, ref_data, char_data):\n",
    "    ref_geometry, ref_progress_percentage, output_size = ref_data\n",
    "    g_data, _, base_data, stroke_sets, _, f_names = char_data\n",
    "    heuristic_scores = {}#[]\n",
    "    for (geometry_length, bases, stroke_set, f_name) in zip(g_data, base_data, stroke_sets, f_names):\n",
    "        #print(f_name)\n",
    "        strokes, p_strokes = geometry_length\n",
    "        base_matrix = strokeErrorMatrix(strokes, ref_geometry, p_strokes, ref_progress_percentage)\n",
    "        #if len(ref_geometry) != len(strokes):\n",
    "        #    print(\"skip\", f_name)\n",
    "        #    continue\n",
    "        # Test through row/col\n",
    "        #print(base_matrix)\n",
    "        error_maps = np.copy(base_matrix)\n",
    "        row_stroke_map = np.full(len(strokes), -1)\n",
    "        col_stroke_map = np.full(len(strokes), -1)\n",
    "        row_mins = np.min(error_maps, axis=1)\n",
    "        col_mins = np.min(error_maps, axis=0)\n",
    "        compare_scores = []\n",
    "        stroke_maps = {}\n",
    "        # Iterate over every smallest error per row\n",
    "        for row_min in range(len(ref_geometry)):\n",
    "            coords = np.argwhere(error_maps == row_mins[row_min])\n",
    "            if len(coords) > 1: # In cases where there are identical error values\n",
    "                for coord in coords:\n",
    "                    if not np.any(row_stroke_map == coord[0]):#row_stroke_map[coord[0]-1] != -1:\n",
    "                        loc = coord\n",
    "                        break\n",
    "            else:\n",
    "                loc = coords[0] # Find [row, col] index of current smallest error\n",
    "            while row_stroke_map[loc[1]] != -1: # Make sure there's no overlap\n",
    "                error_maps[loc[0]][loc[1]] = 10000\n",
    "                loc[1] = np.argmin(error_maps[loc[0]])\n",
    "                # # remind program to switch the priority and repeat\n",
    "            row_stroke_map[loc[1]] = loc[0]\n",
    "            #print(row_stroke_map)\n",
    "        if np.array2string(row_stroke_map) not in stroke_maps:\n",
    "            stroke_maps[np.array2string(row_stroke_map)] = row_stroke_map\n",
    "        # example: row 0's smallest error is at index 2 and so stroke_map[2] = 0\n",
    "        # but row 4's smallest error is also at index 2\n",
    "        # take row 0, recalculate the smallest error excluding index 2,\n",
    "        # but it's too difficult so just permutation of all overlaps and rearrange them\n",
    "        error_maps = np.copy(base_matrix)\n",
    "        for col_min in range(len(ref_geometry)):\n",
    "            coords = np.argwhere(error_maps == col_mins[col_min])\n",
    "            if len(coords) > 1: # In cases where there are identical error values\n",
    "                for coord in coords:\n",
    "                    if col_stroke_map[coord[1]-1] != -1:\n",
    "                        loc = coord\n",
    "                        break\n",
    "            else:\n",
    "                loc = coords[0] # Find [row, col] index of current smallest error\n",
    "            while np.any(col_stroke_map == loc[0]): # Make sure there's no overlap\n",
    "                error_maps[loc[0]][loc[1]] = 10000\n",
    "                loc[0] = np.argmin(error_maps[:, loc[1]])\n",
    "            col_stroke_map[loc[1]] = loc[0]\n",
    "        if np.array2string(col_stroke_map) not in stroke_maps:\n",
    "            stroke_maps[np.array2string(col_stroke_map)] = col_stroke_map\n",
    "        for s in stroke_maps.values():\n",
    "            #print(s)\n",
    "            heuristic_alignment = np.delete(s, np.where(s == -1))+1\n",
    "            heuristic_xml = minXml(ref_char, bases, stroke_set, heuristic_alignment) #5408.2.8.gene returning stroke order containing only 5 elements\n",
    "            try:\n",
    "                heuristic_score = getXmlScore(heuristic_xml)\n",
    "            except:\n",
    "                print(\"err:\", f_name)\n",
    "            compare_scores.append(heuristic_score)\n",
    "        #heuristic_scores.append(max(compare_scores))\n",
    "        heuristic_scores[f_name] = max(compare_scores)\n",
    "    return heuristic_scores\n",
    "\n",
    "def loadRef2(han_char, ref_dir = \"Reference\"):\n",
    "    stroke_list = []\n",
    "    frac_dists = []\n",
    "    ref_path = f\"{ref_dir}/{han_char}.han\"\n",
    "    ref_xml = open(ref_path, \"r\").read()\n",
    "    root = xmltodict.parse(ref_xml)\n",
    "    bounds = root[\"hanDefinition\"][\"bounds\"]\n",
    "    x_min, y_min, x_max, y_max = (float(bounds[\"@left\"]), float(bounds[\"@bottom\"]), float(bounds[\"@right\"]), float(bounds[\"@top\"]))\n",
    "    scale = (int(x_max-x_min), int(y_max-y_min))\n",
    "    strokes = root[\"hanDefinition\"][\"strokes\"][\"stroke\"]\n",
    "    if isinstance(strokes, dict):\n",
    "        strokes = [strokes]\n",
    "    for stroke in strokes:\n",
    "        points = stroke[\"points\"][\"forward\"]\n",
    "        point_arr = []\n",
    "        frac_arr = []\n",
    "        for point in points[\"pointDistance\"]:\n",
    "            point_arr.append((float(point[\"@x\"])-x_min,\n",
    "                               float(point[\"@y\"])-y_min))\n",
    "            frac_arr.append(float(point[\"@fractionalDistance\"]))\n",
    "        stroke_list.append(np.array(point_arr))\n",
    "        frac_dists.append(np.array(frac_arr))\n",
    "    return stroke_list, frac_dists, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf7e9d-44e0-4adb-99cc-94b240d5b447",
   "metadata": {},
   "source": [
    "## Loading genes from the directory and storing scores\n",
    "This will compare all of the genes in \"Genes/sixgenes/test\" with all of the archetypes in \"Reference/6-stroke_characters\"\n",
    "\n",
    "all_scores is a 2-d list that holds all of the score numbers. Unfortunately, each han character is a row instead of a column\n",
    "\n",
    "ref_chars holds all of the han character gene names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc02e5-a7b8-4436-89e8-865391892e39",
   "metadata": {},
   "source": [
    "### Note about loadRef2!\n",
    "Holiday's original code didn't include loadRef2! I made it. It's the exact same as loadRef, except the ref_path doesn't include the {han_char[0]}000 part of it. I did this so I wouldn't have to make all of those files in my test folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a95ef2a6-5103-40f2-bff4-292367203660",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "f_names = []\n",
    "ref_dir = f'{str(Path.home())}/Stylus_Scoring_Generalization/NewRef' # archetype directory\n",
    "for _, _, f_names in os.walk(ref_dir):\n",
    "    for f in f_names:\n",
    "        ref_chars.append(f.split(\".\")[0])\n",
    "    #ref_chars.extend(f.split(\".\")[0] for f in f_names)\n",
    "ref_chars = list(filter(None, ref_chars))\n",
    "if '.ipynb_checkpoints' in ref_chars:\n",
    "    ref_chars.remove('.ipynb_checkpoints')\n",
    "for char in ref_chars:\n",
    "    han_char = char[:4:]\n",
    "    ref_g, ref_l, output_size = loadRef2(han_char, ref_dir)\n",
    "    #if len(ref_g\n",
    "    char_data = loadGeometryBases(data_dir, output_size)\n",
    "    f_names = os.listdir(data_dir)\n",
    "    f_names.sort()\n",
    "    heuristic_scores = heuristic(han_char, [ref_g, ref_l, output_size], char_data).values()\n",
    "    all_scores.append(heuristic_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af13412-466c-4cf4-8200-c7bb6492649a",
   "metadata": {},
   "source": [
    "## Generating a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "32aacabb-c995-4e24-8aa7-0e0ee5c63f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_dir = \"GenXml\"\n",
    "gene_names = []\n",
    "#gene_names = [f_name.split(\".\")[0] for (i, f_name) in enumerate(f_names)]\n",
    "for name in f_names:\n",
    "    x = len(name) - 5\n",
    "    gene_names.append(name[:x])\n",
    "all_scores_t = np.array(all_scores).T\n",
    "#below code makes a label for which gene is which\n",
    "genes = pd.Series(gene_names, name=\"Genes\")\n",
    "f = {}\n",
    "for (char, scores) in zip(ref_chars, all_scores_t):\n",
    "    f[char] = scores\n",
    "#frame = pd.DataFrame.from_records(all_scores_t, columns=ref_chars)\n",
    "frame = pd.DataFrame(f)\n",
    "#this allows the label for the genes to be added to the data frame\n",
    "result = pd.concat([genes, frame], axis=1)\n",
    "table = pd.DataFrame(result)\n",
    "table.to_csv('new_heuristic.csv', index = 'false')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475a1b5-0da3-48b0-a11a-63280f7ce316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
